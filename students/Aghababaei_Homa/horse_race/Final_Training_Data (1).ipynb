{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Delete all uploaded files in the current directory\n",
        "for filename in os.listdir():\n",
        "    if os.path.isfile(filename):\n",
        "        try:\n",
        "            os.remove(filename)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not delete {filename}: {e}\")\n",
        "\n",
        "print(\"All uploaded files have been deleted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ5wtouMTESi",
        "outputId": "33ef0d67-a5d1-4ee3-8d56-fa69ab8052fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All uploaded files have been deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload XML files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Import libraries\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "import re\n",
        "\n",
        "# Robust distance parser\n",
        "def parse_distance(dist_str):\n",
        "    try:\n",
        "        if not dist_str:\n",
        "            return np.nan\n",
        "        dist_str = dist_str.lower()\n",
        "        if 'm' in dist_str:\n",
        "            parts = dist_str.split('m')[0].strip().split()\n",
        "            if len(parts) == 1:\n",
        "                distance = float(parts[0])\n",
        "            else:\n",
        "                whole = float(parts[0])\n",
        "                num, denom = map(float, parts[1].split('/'))\n",
        "                distance = whole + (num / denom)\n",
        "            return distance * 8\n",
        "        elif 'f' in dist_str:\n",
        "            parts = dist_str.split('f')[0].strip().split()\n",
        "            if len(parts) == 1:\n",
        "                return float(parts[0])\n",
        "            else:\n",
        "                whole = float(parts[0])\n",
        "                num, denom = map(float, parts[1].split('/'))\n",
        "                return whole + (num / denom)\n",
        "        else:\n",
        "            return float(dist_str)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# Main parser\n",
        "def parse_past_performance_xml_colab(xml_file, filename=\"unknown.xml\"):\n",
        "    try:\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        all_rows = []\n",
        "\n",
        "        for race in root.findall(\"Race\"):\n",
        "            race_number = race.findtext(\"RaceNumber\")\n",
        "            post_time = race.findtext(\"PostTime\")\n",
        "            distance = race.find(\"Distance/PublishedValue\")\n",
        "            surface = race.find(\"Course/CourseType/Description\")\n",
        "            race_type = race.find(\"RaceType/Description\")\n",
        "            purse = race.findtext(\"PurseUSA\")\n",
        "\n",
        "            distance_val = parse_distance(distance.text) if distance is not None else np.nan\n",
        "            surface_val = surface.text if surface is not None else None\n",
        "            race_type_val = race_type.text if race_type is not None else None\n",
        "            purse_val = float(purse) if purse else np.nan\n",
        "\n",
        "            for starter in race.findall(\"Starters\"):\n",
        "                horse_elem = starter.find(\"Horse\")\n",
        "                if horse_elem is None:\n",
        "                    continue\n",
        "\n",
        "                horse_name = horse_elem.findtext(\"HorseName\")\n",
        "                jockey = starter.find(\"Jockey/LastName\")\n",
        "                trainer = starter.find(\"Trainer/LastName\")\n",
        "                program_number = starter.findtext(\"ProgramNumber\")\n",
        "\n",
        "                current_row = {\n",
        "                    \"RaceID\": f\"{filename}_R{race_number}\",\n",
        "                    \"RaceNumber\": race_number,\n",
        "                    \"PostTime\": post_time,\n",
        "                    \"Distance\": distance_val,\n",
        "                    \"Surface\": surface_val,\n",
        "                    \"RaceType\": race_type_val,\n",
        "                    \"Purse\": purse_val,\n",
        "                    \"HorseName\": horse_name,\n",
        "                    \"Jockey\": jockey.text if jockey is not None else '',\n",
        "                    \"Trainer\": trainer.text if trainer is not None else '',\n",
        "                    \"ProgramNumber\": program_number\n",
        "                }\n",
        "\n",
        "                past_perf_elements = starter.findall('PastPerformance')[:5]\n",
        "                for i, perf in enumerate(past_perf_elements):\n",
        "                    start_elem = perf.find('Start')\n",
        "                    if start_elem is None:\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        race_date = perf.findtext(\"RaceDate\")\n",
        "                        surface = perf.find(\"Course/Surface/Value\")\n",
        "                        distance = perf.find(\"Distance/PublishedValue\")\n",
        "                        purse = perf.findtext(\"PurseUSA\")\n",
        "                        num_starters = perf.findtext(\"NumberOfStarters\")\n",
        "                        track = perf.find(\"Track/TrackID\")\n",
        "                        race_num = perf.find(\"RaceNumber\")\n",
        "\n",
        "                        lengths_last_call = np.nan\n",
        "                        lengths_finish = np.nan\n",
        "                        start_pos = np.nan\n",
        "                        finish_pos = np.nan\n",
        "                        last_call_pos = np.nan\n",
        "\n",
        "                        for call in start_elem.findall(\"PointOfCall\"):\n",
        "                            poc = call.findtext(\"PointOfCall\")\n",
        "                            if poc == 'S':\n",
        "                                pos = call.findtext(\"Position\")\n",
        "                                if pos: start_pos = int(pos)\n",
        "                            elif poc == 'F':\n",
        "                                pos = call.findtext(\"Position\")\n",
        "                                if pos: finish_pos = int(pos)\n",
        "                                if finish_pos != 1:\n",
        "                                    lb = call.findtext(\"LengthsBehind\")\n",
        "                                    if lb: lengths_finish = float(lb)\n",
        "                            if call.findtext(\"PointOfCallPrint\") == 'Y':\n",
        "                                pos = call.findtext(\"Position\")\n",
        "                                if pos: last_call_pos = int(pos)\n",
        "                                lb = call.findtext(\"LengthsBehind\")\n",
        "                                if lb: lengths_last_call = float(lb)\n",
        "\n",
        "                        past_prefix = f\"PP{i+1}_\"\n",
        "                        current_row.update({\n",
        "                            past_prefix + \"RaceID\": f\"{track.text}-{race_date}-R{race_num.text}\" if track is not None and race_num is not None else None,\n",
        "                            past_prefix + \"RaceDate\": race_date,\n",
        "                            past_prefix + \"Surface\": surface.text if surface is not None else None,\n",
        "                            past_prefix + \"Distance\": parse_distance(distance.text) if distance is not None else np.nan,\n",
        "                            past_prefix + \"Purse\": float(purse) if purse else np.nan,\n",
        "                            past_prefix + \"NumStarters\": int(num_starters) if num_starters else np.nan,\n",
        "                            past_prefix + \"FinishPosition\": finish_pos,\n",
        "                            past_prefix + \"LengthsBackFinish\": lengths_finish,\n",
        "                            past_prefix + \"LengthsBackLastCall\": lengths_last_call,\n",
        "                            past_prefix + \"StartPosition\": start_pos,\n",
        "                            past_prefix + \"LastCallPosition\": last_call_pos,\n",
        "                            past_prefix + \"Jockey\": start_elem.findtext(\"Jockey/LastName\") or '',\n",
        "                            past_prefix + \"Trainer\": start_elem.findtext(\"Trainer/LastName\") or '',\n",
        "                        })\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "                all_rows.append(current_row)\n",
        "\n",
        "        return pd.DataFrame(all_rows)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {filename}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Parse all uploaded files\n",
        "all_dfs = []\n",
        "\n",
        "for fname in uploaded:\n",
        "    print(f\"üìÑ Parsing {fname}...\")\n",
        "    df = parse_past_performance_xml_colab(BytesIO(uploaded[fname]), filename=fname)\n",
        "    all_dfs.append(df)\n",
        "\n",
        "full_df = pd.concat(all_dfs, ignore_index=True)\n",
        "full_df[\"DistanceYards\"] = full_df[\"Distance\"] * 220\n",
        "full_df.to_csv(\"training_data_from_xml.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"training_data_from_xml.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "my4aDSMuTgwX",
        "outputId": "984c02fa-ae40-4c0f-ae2c-5a7aa03d9c14",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ac43b826-5302-42dd-811b-0d9317e6c824\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ac43b826-5302-42dd-811b-0d9317e6c824\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving SIMD20230502CD_USA.xml to SIMD20230502CD_USA.xml\n",
            "Saving SIMD20230503CD_USA.xml to SIMD20230503CD_USA.xml\n",
            "Saving SIMD20230504CD_USA.xml to SIMD20230504CD_USA.xml\n",
            "Saving SIMD20230505CD_USA.xml to SIMD20230505CD_USA.xml\n",
            "Saving SIMD20230506CD_USA.xml to SIMD20230506CD_USA.xml\n",
            "Saving SIMD20230511CD_USA.xml to SIMD20230511CD_USA.xml\n",
            "Saving SIMD20230512CD_USA.xml to SIMD20230512CD_USA.xml\n",
            "Saving SIMD20230513CD_USA.xml to SIMD20230513CD_USA.xml\n",
            "Saving SIMD20230514CD_USA.xml to SIMD20230514CD_USA.xml\n",
            "Saving SIMD20230518CD_USA.xml to SIMD20230518CD_USA.xml\n",
            "Saving SIMD20230519CD_USA.xml to SIMD20230519CD_USA.xml\n",
            "Saving SIMD20230520CD_USA.xml to SIMD20230520CD_USA.xml\n",
            "Saving SIMD20230521CD_USA.xml to SIMD20230521CD_USA.xml\n",
            "Saving SIMD20230525CD_USA.xml to SIMD20230525CD_USA.xml\n",
            "Saving SIMD20230526CD_USA.xml to SIMD20230526CD_USA.xml\n",
            "Saving SIMD20230527CD_USA.xml to SIMD20230527CD_USA.xml\n",
            "Saving SIMD20230528CD_USA.xml to SIMD20230528CD_USA.xml\n",
            "Saving SIMD20230529CD_USA.xml to SIMD20230529CD_USA.xml\n",
            "üìÑ Parsing SIMD20230502CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230503CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230504CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230505CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230506CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230511CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230512CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230513CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230514CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230518CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230519CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230520CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230521CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230525CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230526CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230527CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230528CD_USA.xml...\n",
            "üìÑ Parsing SIMD20230529CD_USA.xml...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a9986ea0-ad1e-409b-afcb-3e0deef135cf\", \"training_data_from_xml.csv\", 896326)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload race result XMLs\n",
        "from google.colab import files\n",
        "uploaded_results = files.upload()\n",
        "\n",
        "# Imports\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import re\n",
        "from io import BytesIO\n",
        "\n",
        "# Helpers\n",
        "def create_race_id(track_code, date_str, race_number):\n",
        "    try:\n",
        "        # Ensure date_str is in MM-DD-YY format\n",
        "        month, day, year = date_str.split(\"-\")\n",
        "        year = year[-2:]  # Use last two digits of year\n",
        "        return f\"{track_code}-{month.zfill(2)}-{day.zfill(2)}-{year}-R{int(race_number):02d}\"\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def standardize_race_id(race_id):\n",
        "    try:\n",
        "        # Handle RaceID from features_df (e.g., SIMD20231104CD_USA.xml_R1)\n",
        "        match = re.match(r\"SIMD(\\d{4})(\\d{2})(\\d{2})CD_USA\\.xml_R(\\d+)\", race_id)\n",
        "        if match:\n",
        "            year, month, day, race_num = match.groups()\n",
        "            year = year[-2:]\n",
        "            return f\"CD-{month}-{day}-{year}-R{int(race_num):02d}\"\n",
        "        # Handle RaceID already in CD-MM-DD-YY-RNN format\n",
        "        match = re.match(r\"CD-(\\d{2})-(\\d{2})-(\\d{2})-R(\\d+)\", race_id)\n",
        "        if match:\n",
        "            return race_id  # Already standardized\n",
        "        return race_id\n",
        "    except:\n",
        "        return race_id\n",
        "\n",
        "def normalize_horse_name(name):\n",
        "    if not name or pd.isna(name):\n",
        "        return \"\"\n",
        "    name = str(name).lower().strip()\n",
        "    name = name.replace(\"‚Äô\", \"'\").replace(\"`\", \"'\")  # Handle different apostrophes\n",
        "    name = re.sub(r\"[^a-z0-9'\\s]\", \"\", name)  # Remove all punctuation except apostrophes\n",
        "    name = re.sub(r\"\\s+\", \" \", name)  # Normalize spaces\n",
        "    return name.strip()\n",
        "\n",
        "def parse_race_result_xml(xml_file, filename=\"unknown.xml\"):\n",
        "    try:\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        results = []\n",
        "\n",
        "        races = root.findall(\"RACE\") if root.tag != \"CHART\" else root.findall(\".//RACE\")\n",
        "\n",
        "        for idx, race in enumerate(races):\n",
        "            race_date = race.findtext(\"RACE_DATE\")\n",
        "            race_number = race.findtext(\"RACE_NUMBER\")\n",
        "            track_code = \"CD\"\n",
        "\n",
        "            if not race_date or not race_number:\n",
        "                match = re.match(r\"cd(\\d{8})tch.*\\.xml\", filename)\n",
        "                if match:\n",
        "                    date_str = f\"{match.group(1)[4:6]}-{match.group(1)[6:8]}-{match.group(1)[2:4]}\"\n",
        "                    race_number = str(idx + 1)\n",
        "                    race_date = date_str\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            # Standardize race_date to MM-DD-YY\n",
        "            try:\n",
        "                month, day, year = race_date.split(\"-\")\n",
        "                year = year[-2:]\n",
        "                race_date = f\"{month.zfill(2)}-{day.zfill(2)}-{year}\"\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            race_id = create_race_id(track_code, race_date, race_number)\n",
        "            if not race_id:\n",
        "                continue\n",
        "\n",
        "            for entry in race.findall(\"ENTRY\"):\n",
        "                horse_name = entry.findtext(\"NAME\")\n",
        "                finish = entry.findtext(\"OFFICIAL_FIN\")\n",
        "                if horse_name and finish and finish.isdigit():\n",
        "                    results.append({\n",
        "                        \"RaceID\": race_id,\n",
        "                        \"HorseName\": normalize_horse_name(horse_name),\n",
        "                        \"FinishPosition\": int(finish)\n",
        "                    })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error parsing {filename}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Parse result files\n",
        "all_results = []\n",
        "for fname in uploaded_results:\n",
        "    print(f\"üìÑ Parsing {fname}...\")\n",
        "    df = parse_race_result_xml(BytesIO(uploaded_results[fname]), filename=fname)\n",
        "    all_results.append(df)\n",
        "\n",
        "results_df = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "# Load features\n",
        "features_df = pd.read_csv(\"training_data_from_xml.csv\")\n",
        "features_df[\"RaceID\"] = features_df[\"RaceID\"].apply(standardize_race_id)\n",
        "features_df[\"HorseName\"] = features_df[\"HorseName\"].apply(normalize_horse_name)\n",
        "results_df[\"HorseName\"] = results_df[\"HorseName\"].apply(normalize_horse_name)\n",
        "\n",
        "# Log unique RaceIDs for debugging\n",
        "print(\"Unique RaceIDs in features_df:\", features_df[\"RaceID\"].unique())\n",
        "print(\"Unique RaceIDs in results_df:\", results_df[\"RaceID\"].unique())\n",
        "\n",
        "# Merge with diagnostics\n",
        "merged_df = pd.merge(\n",
        "    features_df,\n",
        "    results_df,\n",
        "    on=[\"RaceID\", \"HorseName\"],\n",
        "    how=\"left\",\n",
        "    indicator=True  # Add merge indicator to track matched/unmatched rows\n",
        ")\n",
        "\n",
        "# Analyze unmatched rows\n",
        "unmatched = merged_df[merged_df[\"_merge\"] == \"left_only\"]\n",
        "print(f\"Number of unmatched rows (potential -1 FinishPosition): {len(unmatched)}\")\n",
        "if len(unmatched) > 0:\n",
        "    print(\"Sample of unmatched rows:\")\n",
        "    print(unmatched[[\"RaceID\", \"HorseName\"]].head())\n",
        "\n",
        "# Instead of filling NaN with -1, drop unmatched rows (or handle differently based on your needs)\n",
        "merged_df = merged_df[merged_df[\"_merge\"] == \"both\"].copy()\n",
        "merged_df.drop(columns=[\"_merge\"], inplace=True)\n",
        "\n",
        "# Ensure FinishPosition is an integer\n",
        "merged_df[\"FinishPosition\"] = merged_df[\"FinishPosition\"].astype(int)\n",
        "\n",
        "# Save\n",
        "merged_df.to_csv(\"final_training_dataset.csv\", index=False)\n",
        "files.download(\"final_training_dataset.csv\")\n",
        "\n",
        "print(f\"‚úÖ Final dataset shape: {merged_df.shape}\")\n",
        "print(\"Finish position distribution:\\n\", merged_df[\"FinishPosition\"].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JljtDlbXTogc",
        "outputId": "cb2c942b-a820-40ea-f165-ffed84367c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cdae8cc0-590d-4891-b9b0-3d16b21dc3ce\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cdae8cc0-590d-4891-b9b0-3d16b21dc3ce\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cd20230502tch.xml to cd20230502tch.xml\n",
            "Saving cd20230503tch.xml to cd20230503tch.xml\n",
            "Saving cd20230504tch.xml to cd20230504tch.xml\n",
            "Saving cd20230505tch.xml to cd20230505tch.xml\n",
            "Saving cd20230506tch.xml to cd20230506tch.xml\n",
            "Saving cd20230511tch.xml to cd20230511tch.xml\n",
            "Saving cd20230512tch.xml to cd20230512tch.xml\n",
            "Saving cd20230513tch.xml to cd20230513tch.xml\n",
            "Saving cd20230514tch.xml to cd20230514tch.xml\n",
            "Saving cd20230518tch.xml to cd20230518tch.xml\n",
            "Saving cd20230519tch.xml to cd20230519tch.xml\n",
            "Saving cd20230520tch.xml to cd20230520tch.xml\n",
            "Saving cd20230521tch.xml to cd20230521tch.xml\n",
            "Saving cd20230525tch.xml to cd20230525tch.xml\n",
            "Saving cd20230526tch.xml to cd20230526tch.xml\n",
            "Saving cd20230527tch.xml to cd20230527tch.xml\n",
            "Saving cd20230528tch.xml to cd20230528tch.xml\n",
            "Saving cd20230529tch.xml to cd20230529tch.xml\n",
            "üìÑ Parsing cd20230502tch.xml...\n",
            "üìÑ Parsing cd20230503tch.xml...\n",
            "üìÑ Parsing cd20230504tch.xml...\n",
            "üìÑ Parsing cd20230505tch.xml...\n",
            "üìÑ Parsing cd20230506tch.xml...\n",
            "üìÑ Parsing cd20230511tch.xml...\n",
            "üìÑ Parsing cd20230512tch.xml...\n",
            "üìÑ Parsing cd20230513tch.xml...\n",
            "üìÑ Parsing cd20230514tch.xml...\n",
            "üìÑ Parsing cd20230518tch.xml...\n",
            "üìÑ Parsing cd20230519tch.xml...\n",
            "üìÑ Parsing cd20230520tch.xml...\n",
            "üìÑ Parsing cd20230521tch.xml...\n",
            "üìÑ Parsing cd20230525tch.xml...\n",
            "üìÑ Parsing cd20230526tch.xml...\n",
            "üìÑ Parsing cd20230527tch.xml...\n",
            "üìÑ Parsing cd20230528tch.xml...\n",
            "üìÑ Parsing cd20230529tch.xml...\n",
            "Unique RaceIDs in features_df: ['CD-05-02-23-R01' 'CD-05-02-23-R02' 'CD-05-02-23-R03' 'CD-05-02-23-R04'\n",
            " 'CD-05-02-23-R05' 'CD-05-02-23-R06' 'CD-05-02-23-R07' 'CD-05-02-23-R08'\n",
            " 'CD-05-02-23-R09' 'CD-05-03-23-R01' 'CD-05-03-23-R02' 'CD-05-03-23-R03'\n",
            " 'CD-05-03-23-R04' 'CD-05-03-23-R05' 'CD-05-03-23-R06' 'CD-05-03-23-R07'\n",
            " 'CD-05-03-23-R08' 'CD-05-03-23-R09' 'CD-05-03-23-R10' 'CD-05-04-23-R01'\n",
            " 'CD-05-04-23-R02' 'CD-05-04-23-R03' 'CD-05-04-23-R04' 'CD-05-04-23-R05'\n",
            " 'CD-05-04-23-R06' 'CD-05-04-23-R07' 'CD-05-04-23-R08' 'CD-05-04-23-R09'\n",
            " 'CD-05-04-23-R10' 'CD-05-04-23-R11' 'CD-05-05-23-R01' 'CD-05-05-23-R02'\n",
            " 'CD-05-05-23-R03' 'CD-05-05-23-R04' 'CD-05-05-23-R05' 'CD-05-05-23-R06'\n",
            " 'CD-05-05-23-R07' 'CD-05-05-23-R08' 'CD-05-05-23-R09' 'CD-05-05-23-R10'\n",
            " 'CD-05-05-23-R11' 'CD-05-05-23-R12' 'CD-05-05-23-R13' 'CD-05-06-23-R01'\n",
            " 'CD-05-06-23-R02' 'CD-05-06-23-R03' 'CD-05-06-23-R04' 'CD-05-06-23-R05'\n",
            " 'CD-05-06-23-R06' 'CD-05-06-23-R07' 'CD-05-06-23-R08' 'CD-05-06-23-R09'\n",
            " 'CD-05-06-23-R10' 'CD-05-06-23-R11' 'CD-05-06-23-R12' 'CD-05-06-23-R13'\n",
            " 'CD-05-06-23-R14' 'CD-05-11-23-R01' 'CD-05-11-23-R02' 'CD-05-11-23-R03'\n",
            " 'CD-05-11-23-R04' 'CD-05-11-23-R05' 'CD-05-11-23-R06' 'CD-05-11-23-R07'\n",
            " 'CD-05-11-23-R08' 'CD-05-12-23-R01' 'CD-05-12-23-R02' 'CD-05-12-23-R03'\n",
            " 'CD-05-12-23-R04' 'CD-05-12-23-R05' 'CD-05-12-23-R06' 'CD-05-12-23-R07'\n",
            " 'CD-05-12-23-R08' 'CD-05-12-23-R09' 'CD-05-13-23-R01' 'CD-05-13-23-R02'\n",
            " 'CD-05-13-23-R03' 'CD-05-13-23-R04' 'CD-05-13-23-R05' 'CD-05-13-23-R06'\n",
            " 'CD-05-13-23-R07' 'CD-05-13-23-R08' 'CD-05-13-23-R09' 'CD-05-13-23-R10'\n",
            " 'CD-05-13-23-R11' 'CD-05-14-23-R01' 'CD-05-14-23-R02' 'CD-05-14-23-R03'\n",
            " 'CD-05-14-23-R04' 'CD-05-14-23-R05' 'CD-05-14-23-R06' 'CD-05-14-23-R07'\n",
            " 'CD-05-14-23-R08' 'CD-05-14-23-R09' 'CD-05-14-23-R10' 'CD-05-18-23-R01'\n",
            " 'CD-05-18-23-R02' 'CD-05-18-23-R03' 'CD-05-18-23-R04' 'CD-05-18-23-R05'\n",
            " 'CD-05-18-23-R06' 'CD-05-18-23-R07' 'CD-05-18-23-R08' 'CD-05-19-23-R01'\n",
            " 'CD-05-19-23-R02' 'CD-05-19-23-R03' 'CD-05-19-23-R04' 'CD-05-19-23-R05'\n",
            " 'CD-05-19-23-R06' 'CD-05-19-23-R07' 'CD-05-19-23-R08' 'CD-05-19-23-R09'\n",
            " 'CD-05-19-23-R10' 'CD-05-20-23-R01' 'CD-05-20-23-R02' 'CD-05-20-23-R03'\n",
            " 'CD-05-20-23-R04' 'CD-05-20-23-R05' 'CD-05-20-23-R06' 'CD-05-20-23-R07'\n",
            " 'CD-05-20-23-R08' 'CD-05-20-23-R09' 'CD-05-20-23-R10' 'CD-05-20-23-R11'\n",
            " 'CD-05-21-23-R01' 'CD-05-21-23-R02' 'CD-05-21-23-R03' 'CD-05-21-23-R04'\n",
            " 'CD-05-21-23-R05' 'CD-05-21-23-R06' 'CD-05-21-23-R07' 'CD-05-21-23-R08'\n",
            " 'CD-05-21-23-R09' 'CD-05-25-23-R01' 'CD-05-25-23-R02' 'CD-05-25-23-R03'\n",
            " 'CD-05-25-23-R04' 'CD-05-25-23-R05' 'CD-05-25-23-R06' 'CD-05-25-23-R07'\n",
            " 'CD-05-25-23-R08' 'CD-05-26-23-R01' 'CD-05-26-23-R02' 'CD-05-26-23-R03'\n",
            " 'CD-05-26-23-R04' 'CD-05-26-23-R05' 'CD-05-26-23-R06' 'CD-05-26-23-R07'\n",
            " 'CD-05-26-23-R08' 'CD-05-26-23-R09' 'CD-05-27-23-R01' 'CD-05-27-23-R02'\n",
            " 'CD-05-27-23-R03' 'CD-05-27-23-R04' 'CD-05-27-23-R05' 'CD-05-27-23-R06'\n",
            " 'CD-05-27-23-R07' 'CD-05-27-23-R08' 'CD-05-27-23-R09' 'CD-05-27-23-R10'\n",
            " 'CD-05-27-23-R11' 'CD-05-28-23-R01' 'CD-05-28-23-R02' 'CD-05-28-23-R03'\n",
            " 'CD-05-28-23-R04' 'CD-05-28-23-R05' 'CD-05-28-23-R06' 'CD-05-28-23-R07'\n",
            " 'CD-05-28-23-R08' 'CD-05-28-23-R09' 'CD-05-29-23-R01' 'CD-05-29-23-R02'\n",
            " 'CD-05-29-23-R03' 'CD-05-29-23-R04' 'CD-05-29-23-R05' 'CD-05-29-23-R06'\n",
            " 'CD-05-29-23-R07' 'CD-05-29-23-R08' 'CD-05-29-23-R09']\n",
            "Unique RaceIDs in results_df: ['CD-05-02-23-R01' 'CD-05-02-23-R02' 'CD-05-02-23-R03' 'CD-05-02-23-R04'\n",
            " 'CD-05-02-23-R05' 'CD-05-02-23-R06' 'CD-05-02-23-R07' 'CD-05-02-23-R08'\n",
            " 'CD-05-02-23-R09' 'CD-05-03-23-R01' 'CD-05-03-23-R02' 'CD-05-03-23-R03'\n",
            " 'CD-05-03-23-R04' 'CD-05-03-23-R05' 'CD-05-03-23-R06' 'CD-05-03-23-R07'\n",
            " 'CD-05-03-23-R08' 'CD-05-03-23-R09' 'CD-05-03-23-R10' 'CD-05-04-23-R01'\n",
            " 'CD-05-04-23-R02' 'CD-05-04-23-R03' 'CD-05-04-23-R04' 'CD-05-04-23-R05'\n",
            " 'CD-05-04-23-R06' 'CD-05-04-23-R07' 'CD-05-04-23-R08' 'CD-05-04-23-R09'\n",
            " 'CD-05-04-23-R10' 'CD-05-04-23-R11' 'CD-05-05-23-R01' 'CD-05-05-23-R02'\n",
            " 'CD-05-05-23-R03' 'CD-05-05-23-R04' 'CD-05-05-23-R05' 'CD-05-05-23-R06'\n",
            " 'CD-05-05-23-R07' 'CD-05-05-23-R08' 'CD-05-05-23-R09' 'CD-05-05-23-R10'\n",
            " 'CD-05-05-23-R11' 'CD-05-05-23-R12' 'CD-05-05-23-R13' 'CD-05-06-23-R01'\n",
            " 'CD-05-06-23-R02' 'CD-05-06-23-R03' 'CD-05-06-23-R04' 'CD-05-06-23-R05'\n",
            " 'CD-05-06-23-R06' 'CD-05-06-23-R07' 'CD-05-06-23-R08' 'CD-05-06-23-R09'\n",
            " 'CD-05-06-23-R10' 'CD-05-06-23-R11' 'CD-05-06-23-R12' 'CD-05-06-23-R13'\n",
            " 'CD-05-06-23-R14' 'CD-05-11-23-R01' 'CD-05-11-23-R02' 'CD-05-11-23-R03'\n",
            " 'CD-05-11-23-R04' 'CD-05-11-23-R05' 'CD-05-11-23-R06' 'CD-05-11-23-R07'\n",
            " 'CD-05-11-23-R08' 'CD-05-12-23-R01' 'CD-05-12-23-R02' 'CD-05-12-23-R03'\n",
            " 'CD-05-12-23-R04' 'CD-05-12-23-R05' 'CD-05-12-23-R06' 'CD-05-12-23-R07'\n",
            " 'CD-05-12-23-R08' 'CD-05-12-23-R09' 'CD-05-13-23-R01' 'CD-05-13-23-R02'\n",
            " 'CD-05-13-23-R03' 'CD-05-13-23-R04' 'CD-05-13-23-R05' 'CD-05-13-23-R06'\n",
            " 'CD-05-13-23-R07' 'CD-05-13-23-R08' 'CD-05-13-23-R09' 'CD-05-13-23-R10'\n",
            " 'CD-05-13-23-R11' 'CD-05-14-23-R01' 'CD-05-14-23-R02' 'CD-05-14-23-R03'\n",
            " 'CD-05-14-23-R04' 'CD-05-14-23-R05' 'CD-05-14-23-R06' 'CD-05-14-23-R07'\n",
            " 'CD-05-14-23-R08' 'CD-05-14-23-R09' 'CD-05-14-23-R10' 'CD-05-18-23-R01'\n",
            " 'CD-05-18-23-R02' 'CD-05-18-23-R03' 'CD-05-18-23-R04' 'CD-05-18-23-R05'\n",
            " 'CD-05-18-23-R06' 'CD-05-18-23-R07' 'CD-05-18-23-R08' 'CD-05-19-23-R01'\n",
            " 'CD-05-19-23-R02' 'CD-05-19-23-R03' 'CD-05-19-23-R04' 'CD-05-19-23-R05'\n",
            " 'CD-05-19-23-R06' 'CD-05-19-23-R07' 'CD-05-19-23-R08' 'CD-05-19-23-R09'\n",
            " 'CD-05-19-23-R10' 'CD-05-20-23-R01' 'CD-05-20-23-R02' 'CD-05-20-23-R03'\n",
            " 'CD-05-20-23-R04' 'CD-05-20-23-R05' 'CD-05-20-23-R06' 'CD-05-20-23-R07'\n",
            " 'CD-05-20-23-R08' 'CD-05-20-23-R09' 'CD-05-20-23-R10' 'CD-05-20-23-R11'\n",
            " 'CD-05-21-23-R01' 'CD-05-21-23-R02' 'CD-05-21-23-R03' 'CD-05-21-23-R04'\n",
            " 'CD-05-21-23-R05' 'CD-05-21-23-R06' 'CD-05-21-23-R07' 'CD-05-21-23-R08'\n",
            " 'CD-05-21-23-R09' 'CD-05-25-23-R01' 'CD-05-25-23-R02' 'CD-05-25-23-R03'\n",
            " 'CD-05-25-23-R04' 'CD-05-25-23-R05' 'CD-05-25-23-R06' 'CD-05-25-23-R07'\n",
            " 'CD-05-25-23-R08' 'CD-05-26-23-R01' 'CD-05-26-23-R02' 'CD-05-26-23-R03'\n",
            " 'CD-05-26-23-R04' 'CD-05-26-23-R05' 'CD-05-26-23-R06' 'CD-05-26-23-R07'\n",
            " 'CD-05-26-23-R08' 'CD-05-26-23-R09' 'CD-05-27-23-R01' 'CD-05-27-23-R02'\n",
            " 'CD-05-27-23-R03' 'CD-05-27-23-R04' 'CD-05-27-23-R05' 'CD-05-27-23-R06'\n",
            " 'CD-05-27-23-R07' 'CD-05-27-23-R08' 'CD-05-27-23-R09' 'CD-05-27-23-R10'\n",
            " 'CD-05-27-23-R11' 'CD-05-28-23-R01' 'CD-05-28-23-R02' 'CD-05-28-23-R03'\n",
            " 'CD-05-28-23-R04' 'CD-05-28-23-R05' 'CD-05-28-23-R06' 'CD-05-28-23-R07'\n",
            " 'CD-05-28-23-R08' 'CD-05-28-23-R09' 'CD-05-29-23-R01' 'CD-05-29-23-R02'\n",
            " 'CD-05-29-23-R03' 'CD-05-29-23-R04' 'CD-05-29-23-R05' 'CD-05-29-23-R06'\n",
            " 'CD-05-29-23-R07' 'CD-05-29-23-R08' 'CD-05-29-23-R09']\n",
            "Number of unmatched rows (potential -1 FinishPosition): 287\n",
            "Sample of unmatched rows:\n",
            "             RaceID           HorseName\n",
            "2   CD-05-02-23-R01  toto'srubyslippers\n",
            "8   CD-05-02-23-R02     party at grants\n",
            "11  CD-05-02-23-R02   lotsandlotsofgold\n",
            "30  CD-05-02-23-R04  a little bit crazy\n",
            "38  CD-05-02-23-R05             sake fr\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7ceb00fa-6a44-41e1-b199-52e365afb962\", \"final_training_dataset.csv\", 735514)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Final dataset shape: (1470, 78)\n",
            "Finish position distribution:\n",
            " FinishPosition\n",
            "1     177\n",
            "2     175\n",
            "3     177\n",
            "4     178\n",
            "5     176\n",
            "6     160\n",
            "7     134\n",
            "8     104\n",
            "9      71\n",
            "10     55\n",
            "11     33\n",
            "12     19\n",
            "13      5\n",
            "14      2\n",
            "15      1\n",
            "16      1\n",
            "17      1\n",
            "18      1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the final dataset\n",
        "df = pd.read_csv(\"final_training_dataset.csv\")\n",
        "\n",
        "# Count unique races\n",
        "num_races = df[\"RaceID\"].nunique()\n",
        "\n",
        "# Count total entries (each row is an entry)\n",
        "num_entries = len(df)\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of unique races: {num_races}\")\n",
        "print(f\"Total number of entries: {num_entries}\")\n",
        "\n",
        "# Optional: Average entries per race\n",
        "if num_races > 0:\n",
        "    avg_entries_per_race = num_entries / num_races\n",
        "    print(f\"Average entries per race: {avg_entries_per_race:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-0kcbW1VaVy",
        "outputId": "710f5886-872d-4317-a03b-686036e1095a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique races: 179\n",
            "Total number of entries: 1470\n",
            "Average entries per race: 8.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"final_training_dataset.csv\")\n",
        "\n",
        "# --- Step 1: Standardize Surface ---\n",
        "for col in [col for col in df.columns if \"Surface\" in col]:\n",
        "    df[col] = df[col].replace({\"D\": \"Dirt\", \"T\": \"Turf\"})\n",
        "\n",
        "# --- Step 2: Cap LengthsBack outliers at 100.0 ---\n",
        "for col in [col for col in df.columns if \"LengthsBack\" in col]:\n",
        "    df[col] = df[col].apply(lambda x: min(x, 100.0) if pd.notna(x) else x)\n",
        "\n",
        "# --- Step 3: Handle missing past performance data ---\n",
        "numerical_pp_cols = [col for col in df.columns if col.startswith(\"PP\") and\n",
        "                    (\"Distance\" in col or \"Purse\" in col or \"Lengths\" in col or\n",
        "                     \"NumStarters\" in col or \"Position\" in col)]\n",
        "categorical_pp_cols = [col for col in df.columns if col.startswith(\"PP\") and\n",
        "                      (\"Surface\" in col or \"Jockey\" in col or \"Trainer\" in col)]\n",
        "\n",
        "df[numerical_pp_cols] = df[numerical_pp_cols].fillna(0)\n",
        "df[categorical_pp_cols] = df[categorical_pp_cols].fillna(\"Unknown\")\n",
        "\n",
        "# --- Step 4: Feature Engineering ---\n",
        "\n",
        "# 1. AvgPastFinishPosition (treat 0 as missing)\n",
        "pp_finish_cols = [f\"PP{i}_FinishPosition\" for i in range(1, 6)]\n",
        "df[\"AvgPastFinishPosition\"] = df[pp_finish_cols].applymap(lambda x: np.nan if x == 0 else x).mean(axis=1).fillna(0)\n",
        "\n",
        "# 2. WinRate (fraction of past races won)\n",
        "df[\"WinRate\"] = df[pp_finish_cols].applymap(lambda x: np.nan if x == 0 else x).eq(1).sum(axis=1) / \\\n",
        "                df[pp_finish_cols].applymap(lambda x: np.nan if x == 0 else x).notna().sum(axis=1)\n",
        "df[\"WinRate\"] = df[\"WinRate\"].fillna(0)\n",
        "\n",
        "# 3. DistanceSuitability\n",
        "pp_distance_cols = [f\"PP{i}_Distance\" for i in range(1, 6)]\n",
        "df[\"DistanceSuitability\"] = df[pp_distance_cols].sub(df[\"Distance\"], axis=0).abs().mean(axis=1).fillna(0)\n",
        "\n",
        "# 4. JockeyWinRate (global average win rate per jockey)\n",
        "df[\"JockeyWinRate\"] = df.groupby(\"Jockey\")[\"FinishPosition\"].transform(lambda x: (x == 1).mean())\n",
        "\n",
        "# 5. FieldStrength (average past finish ability of other horses in the race)\n",
        "df[\"FieldStrength\"] = df.groupby(\"RaceID\")[\"AvgPastFinishPosition\"].transform(\"mean\")\n",
        "\n",
        "# 6. SurfaceWinRate\n",
        "pp_surface_cols = [f\"PP{i}_Surface\" for i in range(1, 6)]\n",
        "def surface_win_rate(row):\n",
        "    matches = 0\n",
        "    total = 0\n",
        "    for i in range(1, 6):\n",
        "        surf = row.get(f\"PP{i}_Surface\")\n",
        "        win = row.get(f\"PP{i}_FinishPosition\")\n",
        "        if pd.notna(surf) and surf != \"Unknown\":\n",
        "            total += 1\n",
        "            if win == 1 and surf == row[\"Surface\"]:\n",
        "                matches += 1\n",
        "    return matches / total if total > 0 else 0\n",
        "df[\"SurfaceWinRate\"] = df.apply(surface_win_rate, axis=1)\n",
        "\n",
        "# --- Step 5: Normalize Race Distance ---\n",
        "df[\"Distance\"] = (df[\"Distance\"] - df[\"Distance\"].min()) / (df[\"Distance\"].max() - df[\"Distance\"].min())\n",
        "\n",
        "# --- Step 6: One-hot encode Surface ---\n",
        "df = pd.get_dummies(df, columns=[\"Surface\"], prefix=\"Surface\")\n",
        "\n",
        "# --- Step 7: Select final features ---\n",
        "selected_columns = [\n",
        "    \"HorseName\", \"RaceID\", \"FinishPosition\",  # Target = FinishPosition\n",
        "    \"Distance\", \"Surface_Dirt\", \"Surface_Turf\",\n",
        "    \"AvgPastFinishPosition\", \"WinRate\", \"DistanceSuitability\",\n",
        "    \"JockeyWinRate\", \"FieldStrength\", \"SurfaceWinRate\"\n",
        "]\n",
        "final_df = df[selected_columns]\n",
        "\n",
        "# --- Step 8: Save to CSV ---\n",
        "final_df.to_csv(\"selected_features_dataset.csv\", index=False)\n",
        "\n",
        "# --- Summary ---\n",
        "print(\"‚úÖ Dataset created for FinishPosition regression.\")\n",
        "print(f\"Shape: {final_df.shape}\")\n",
        "print(\"Columns:\", final_df.columns.tolist())\n",
        "print(\"\\nSample rows:\")\n",
        "print(final_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE6D3D8E62Lu",
        "outputId": "4740bd09-771a-41b1-e640-f27dfaebcd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset created for FinishPosition regression.\n",
            "Shape: (1470, 12)\n",
            "Columns: ['HorseName', 'RaceID', 'FinishPosition', 'Distance', 'Surface_Dirt', 'Surface_Turf', 'AvgPastFinishPosition', 'WinRate', 'DistanceSuitability', 'JockeyWinRate', 'FieldStrength', 'SurfaceWinRate']\n",
            "\n",
            "Sample rows:\n",
            "            HorseName           RaceID  FinishPosition  Distance  \\\n",
            "0        gormleyesque  CD-05-02-23-R01               2  0.333333   \n",
            "1      kentucky reign  CD-05-02-23-R01               3  0.333333   \n",
            "2  dogwoodsmilliejane  CD-05-02-23-R01               1  0.333333   \n",
            "3         girls house  CD-05-02-23-R01               4  0.333333   \n",
            "4              recite  CD-05-02-23-R01               5  0.333333   \n",
            "\n",
            "   Surface_Dirt  Surface_Turf  AvgPastFinishPosition  WinRate  \\\n",
            "0          True         False               6.750000      0.0   \n",
            "1          True         False               8.000000      0.0   \n",
            "2          True         False               7.000000      0.0   \n",
            "3          True         False               4.400000      0.0   \n",
            "4          True         False               7.333333      0.0   \n",
            "\n",
            "   DistanceSuitability  JockeyWinRate  FieldStrength  SurfaceWinRate  \n",
            "0                  2.2       0.033333       7.013889             0.0  \n",
            "1                  2.0       0.100000       7.013889             0.0  \n",
            "2                  0.6       0.200000       7.013889             0.0  \n",
            "3                  0.9       0.180451       7.013889             0.0  \n",
            "4                  3.2       0.120000       7.013889             0.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-f66f99145d30>:29: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df[\"AvgPastFinishPosition\"] = df[pp_finish_cols].applymap(lambda x: np.nan if x == 0 else x).mean(axis=1).fillna(0)\n",
            "<ipython-input-14-f66f99145d30>:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df[\"WinRate\"] = df[pp_finish_cols].applymap(lambda x: np.nan if x == 0 else x).eq(1).sum(axis=1) / \\\n",
            "<ipython-input-14-f66f99145d30>:33: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df[pp_finish_cols].applymap(lambda x: np.nan if x == 0 else x).notna().sum(axis=1)\n"
          ]
        }
      ]
    }
  ]
}