# -*- coding: utf-8 -*-
"""Week12 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19ev0tX0MFCTe_6H_deZwLX8WOLCBbG9H
"""

# Commented out IPython magic to ensure Python compatibility.
# Clone the GitHub repo
!git clone https://github.com/Masoudn71/fsan830spring2025.git

# Navigate to the folder with your data
# %cd fsan830spring2025/data/rawDataForTraining

import os

xml_path = "/content/fsan830spring2025/data/rawDataForTraining/pastPerformanceData"
files = os.listdir(xml_path)

for f in files:
    print(f)

"""
This script processes XML racing data from the following locations:
- Input: ../data/rawDataForTraining/pastPerformanceData/*.xml (raw XML racing data files)
- Output: ../data/processed/processed_race_data.nc (processed data in netCDF format)
The script parses XML files containing past performance data and creates a structured dataset
for analysis.
"""

import xml.etree.ElementTree as ET
import xarray as xr
import numpy as np
import os
from pathlib import Path
import pandas as pd
from collections import defaultdict
from datetime import datetime

def convert_fractional_odds(odds_str):
    """Convert fractional odds (e.g., '6/1') to decimal odds."""
    try:
        if not odds_str or odds_str == '0':
            return 0.0
        if '/' in odds_str:
            numerator, denominator = odds_str.split('/')
            return float(numerator) / float(denominator) + 1
        return float(odds_str)
    except (ValueError, ZeroDivisionError):
        return 0.0

def parse_distance(distance_str):
    """Convert distance string (e.g., '6f', '1 1/16m') to furlongs as float."""
    try:
        if not distance_str:
            return np.nan

        # Convert to lowercase for case-insensitive matching
        distance_str = distance_str.lower()

        # Split the string into number and unit
        if 'm' in distance_str:
            # Handle miles (e.g., '1 1/16m', '1m')
            parts = distance_str.split('m')[0].strip().split()
            if len(parts) == 1:
                # Whole miles (e.g., '1m')
                distance = float(parts[0])
            else:
                # Fractional miles (e.g., '1 1/16m')
                whole = float(parts[0])
                fraction = parts[1]
                if '/' in fraction:
                    num, denom = map(float, fraction.split('/'))
                    distance = whole + (num / denom)
                else:
                    distance = whole
            # Convert miles to furlongs (1 mile = 8 furlongs)
            distance *= 8
        elif 'f' in distance_str:
            # Handle furlongs (e.g., '6f', '5 1/2f')
            parts = distance_str.split('f')[0].strip().split()
            if len(parts) == 1:
                # Whole furlongs (e.g., '6f')
                distance = float(parts[0])
            else:
                # Fractional furlongs (e.g., '5 1/2f')
                whole = float(parts[0])
                fraction = parts[1]
                if '/' in fraction:
                    num, denom = map(float, fraction.split('/'))
                    distance = whole + (num / denom)
                else:
                    distance = whole
        else:
            return np.nan

        return distance
    except (ValueError, IndexError):
        return np.nan

def parse_past_performance_xml(xml_file):
    """Parse a past performance XML file and extract current race and past performance information."""
    try:
        # Try to parse the XML file with error handling
        try:
            tree = ET.parse(xml_file)
        except ET.ParseError as e:
            print(f"XML parsing error in {xml_file.name}: {str(e)}")
            return None

        root = tree.getroot()

        # Print the root tag to verify we're reading the XML correctly
        print(f"Root tag: {root.tag}")

        # Get current race date and track from filename
        filename = xml_file.name
        try:
            date_str = filename[4:12]  # Extract YYYYMMDD
            current_year = date_str[:4]
            current_month = date_str[4:6]
            current_day = date_str[6:8]
            current_year_short = current_year[-2:]
            current_track = filename[12:14]  # Extract track code
        except:
            print(f"Warning: Could not parse date/track from filename {filename}")
            return None

        # Initialize data structures for current races
        current_races = {
            'race_ids': [],
            'surfaces': [],
            'distances': [],
            'purses': [],
            'class_ratings': [],
            'horses': [],
            'jockeys': [],
            'trainers': [],
            'program_numbers': []
        }

        # Initialize data structures for past performances
        past_performances = {
            'horse': [],
            'recent_race_ids': [],
            'recent_finish_positions': [],
            'recent_lengths_back_finish': [],
            'recent_lengths_back_last_call': [],
            'recent_speed_figs': [],
            'recent_surfaces': [],
            'recent_distances': [],
            'recent_dates': [],
            'recent_purses': [],
            'recent_start_positions': [],
            'recent_numStarters': [],
            'recent_jockeys': [],
            'recent_trainers': [],
            'recent_last_call_positions': []
        }

        # Process each race in the current card
        race_elements = root.findall('.//Race')
        print(f"Found {len(race_elements)} race elements in {xml_file.name}")

        for race_idx, race in enumerate(race_elements):
            try:
                # Get race number
                race_number_elem = race.find('RaceNumber')
                race_number = race_number_elem.text if race_number_elem is not None else ''

                # Create current race ID
                current_race_id = f"{current_track}-{current_month}-{current_day}-{current_year_short}-R{int(race_number):02d}"

                # Get surface
                course = race.find('Course')
                surface = 'Unknown'
                if course is not None:
                    surface_elem = course.find('Surface/Value')
                    if surface_elem is not None:
                        surface = surface_elem.text

                # Get distance
                distance_elem = race.find('Distance/PublishedValue')
                distance = parse_distance(distance_elem.text) if distance_elem is not None else np.nan

                # Get purse and class rating
                purse_elem = race.find('PurseUSA')
                purse = float(purse_elem.text) if purse_elem is not None else 0.0

                # Process each starter in the current race
                starter_elements = race.findall('Starters')
                for starter_idx, starter in enumerate(starter_elements):
                    try:
                        # Get horse info
                        horse_elem = starter.find('Horse')
                        horse = horse_elem.find('HorseName').text if horse_elem is not None and horse_elem.find('HorseName') is not None else ''

                        # Get jockey info
                        jockey_elem = starter.find('Jockey')
                        jockey = jockey_elem.find('LastName').text if jockey_elem is not None and jockey_elem.find('LastName') is not None else ''

                        # Get trainer info
                        trainer_elem = starter.find('Trainer')
                        trainer = trainer_elem.find('LastName').text if trainer_elem is not None and trainer_elem.find('LastName') is not None else ''

                        # Get program number
                        program_number = starter.find('ProgramNumber').text if starter.find('ProgramNumber') is not None else ''

                        # Store current race data
                        current_races['race_ids'].append(current_race_id)
                        current_races['surfaces'].append(surface)
                        current_races['distances'].append(distance)
                        current_races['purses'].append(purse)
                        current_races['horses'].append(horse)
                        current_races['jockeys'].append(jockey)
                        current_races['trainers'].append(trainer)
                        current_races['program_numbers'].append(program_number)

                        # Process past performances for this horse
                        past_races = []
                        past_start_positions = []
                        past_finish_positions = []
                        past_lengths_back_finish = []
                        past_lengths_back_last_call = []
                        past_last_call_positions = []
                        past_surfaces = []
                        past_distances = []
                        past_dates = []
                        past_purses = []
                        past_numStarters = []
                        past_jockeys = []
                        past_trainers = []

                        # Get past performances (up to 5 most recent)
                        past_perf_elements = starter.findall('PastPerformance')

                        for perf in past_perf_elements[:5]:  # Only take the 5 most recent
                            try:
                                # Get the Start element which contains the race details
                                start_elem = perf.find('Start')
                                if start_elem is None:
                                    continue

                                # Get lengths back at finish and last printed call point
                                lengths_finish = 0.0
                                lengths_last_call = 0.0
                                start_position = 0
                                finish_position = 0
                                last_call_position = 0

                                point_of_calls = start_elem.findall('PointOfCall')
                                last_printed_call = None

                                for call in point_of_calls:
                                    point_of_call = call.find('PointOfCall').text
                                    if point_of_call == 'S':
                                        # Get starting position
                                        position = call.find('Position')
                                        if position is not None:
                                            start_position = int(position.text)
                                    elif point_of_call == 'F':
                                        # Get finish position and lengths behind at finish
                                        position = call.find('Position')
                                        if position is not None:
                                            finish_position = int(position.text)
                                            # If horse is in first position, lengths behind should be 0
                                            if finish_position == 1:
                                                lengths_finish = 0.0
                                            else:
                                                lengths_behind = call.find('LengthsBehind')
                                                if lengths_behind is not None:
                                                    lengths_finish = float(lengths_behind.text)
                                        else:
                                            lengths_behind = call.find('LengthsBehind')
                                            if lengths_behind is not None:
                                                lengths_finish = float(lengths_behind.text)
                                    elif call.find('PointOfCallPrint').text == 'Y':
                                        # Track the last printed call point
                                        last_printed_call = call
                                        position = call.find('Position')
                                        if position is not None:
                                            last_call_position = int(position.text)

                                # Get lengths behind at last printed call point
                                if last_printed_call is not None:
                                    lengths_behind = last_printed_call.find('LengthsBehind')
                                    if lengths_behind is not None:
                                        lengths_last_call = float(lengths_behind.text)

                                # Get number of starters
                                num_starters = int(perf.find('NumberOfStarters').text) if perf.find('NumberOfStarters') is not None else 0

                                # Get surface and distance
                                perf_surface = perf.find('Course/Surface/Value').text if perf.find('Course/Surface/Value') is not None else 'Unknown'
                                perf_distance_elem = perf.find('Distance/PublishedValue')
                                if perf_distance_elem is not None:
                                    perf_distance = parse_distance(perf_distance_elem.text)
                                else:
                                    perf_distance = np.nan

                                # Get purse
                                perf_purse = float(perf.find('PurseUSA').text) if perf.find('PurseUSA') is not None else 0.0

                                # Get race date
                                perf_date = perf.find('RaceDate').text
                                perf_date_parts = perf_date.split('-')
                                perf_year = perf_date_parts[2][-2:]
                                perf_month = perf_date_parts[0]
                                perf_day = perf_date_parts[1]

                                # Get track and race number
                                perf_track = perf.find('Track/TrackID').text if perf.find('Track/TrackID') is not None else ''
                                perf_race_num = perf.find('RaceNumber').text if perf.find('RaceNumber') is not None else ''

                                # Get jockey and trainer info
                                perf_jockey = start_elem.find('Jockey/LastName').text if start_elem.find('Jockey/LastName') is not None else ''
                                perf_trainer = start_elem.find('Trainer/LastName').text if start_elem.find('Trainer/LastName') is not None else ''

                                # Create past race ID
                                past_race_id = f"{perf_track}-{perf_month}-{perf_day}-{perf_year}-R{int(perf_race_num):02d}" if perf_track and perf_race_num else None

                                past_races.append(past_race_id)
                                past_finish_positions.append(finish_position)  # Store actual finish position
                                past_lengths_back_finish.append(lengths_finish)  # Store lengths behind at finish
                                past_lengths_back_last_call.append(lengths_last_call)  # Store lengths behind at last printed call
                                past_last_call_positions.append(last_call_position)  # Store position at last printed call
                                past_surfaces.append(perf_surface)
                                past_distances.append(perf_distance)
                                past_dates.append(perf_date)
                                past_purses.append(perf_purse)
                                past_start_positions.append(start_position)
                                past_numStarters.append(num_starters)
                                past_jockeys.append(perf_jockey)
                                past_trainers.append(perf_trainer)

                            except Exception as e:
                                print(f"Error processing past performance for horse {horse}: {str(e)}")
                                continue

                        # Pad with None if less than 5 races
                        while len(past_races) < 5:
                            past_races.append(None)
                            past_finish_positions.append(None)
                            past_lengths_back_finish.append(None)
                            past_lengths_back_last_call.append(None)
                            past_last_call_positions.append(None)
                            past_surfaces.append(None)
                            past_distances.append(None)
                            past_dates.append(None)
                            past_purses.append(None)
                            past_start_positions.append(None)
                            past_numStarters.append(None)
                            past_jockeys.append(None)
                            past_trainers.append(None)

                        # Store past performance data
                        past_performances['horse'].append(horse)
                        past_performances['recent_race_ids'].append(past_races)
                        past_performances['recent_finish_positions'].append(past_finish_positions)
                        past_performances['recent_lengths_back_finish'].append(past_lengths_back_finish)
                        past_performances['recent_lengths_back_last_call'].append(past_lengths_back_last_call)
                        past_performances['recent_last_call_positions'].append(past_last_call_positions)
                        past_performances['recent_surfaces'].append(past_surfaces)
                        past_performances['recent_distances'].append(past_distances)
                        past_performances['recent_dates'].append(past_dates)
                        past_performances['recent_purses'].append(past_purses)
                        past_performances['recent_start_positions'].append(past_start_positions)
                        past_performances['recent_numStarters'].append(past_numStarters)
                        past_performances['recent_jockeys'].append(past_jockeys)
                        past_performances['recent_trainers'].append(past_trainers)

                    except Exception as e:
                        print(f"Error processing starter {starter_idx + 1}: {str(e)}")
                        continue

            except Exception as e:
                print(f"Error processing race {race_idx + 1}: {str(e)}")
                continue

        print(f"Extracted data for {len(current_races['race_ids'])} current races from {xml_file.name}")
        print(f"Extracted past performances for {len(past_performances['horse'])} horses")

        return {
            'current_races': current_races,
            'past_performances': past_performances
        }
    except Exception as e:
        print(f"Error processing {xml_file.name}: {str(e)}")
        return None

def create_xarray_dataset(data_dir):
    """Create an xarray dataset from all XML files in the directory."""
    # Initialize data structures
    current_races = {
        'race_ids': [],
        'surfaces': [],
        'distances': [],
        'purses': [],
        'class_ratings': [],
        'horses': [],
        'jockeys': [],
        'trainers': [],
        'program_numbers': []
    }

    past_performances = {
        'horse': [],
        'recent_race_ids': [],
        'recent_finish_positions': [],
        'recent_lengths_back_finish': [],
        'recent_lengths_back_last_call': [],
        'recent_speed_figs': [],
        'recent_surfaces': [],
        'recent_distances': [],
        'recent_dates': [],
        'recent_purses': [],
        'recent_start_positions': [],
        'recent_numStarters': [],
        'recent_jockeys': [],
        'recent_trainers': [],
        'recent_last_call_positions': []
    }

    # Process all XML files in the directory
    data_dir_path = Path(data_dir)
    if not data_dir_path.exists():
        raise FileNotFoundError(f"Data directory not found: {data_dir}")

    xml_files = list(data_dir_path.glob('*.xml'))
    if not xml_files:
        raise FileNotFoundError(f"No XML files found in directory: {data_dir}")

    print(f"Found {len(xml_files)} XML files to process")

    # Track statistics for each file
    file_stats = {}

    for xml_file in xml_files:
        print(f"\nProcessing {xml_file.name}...")
        data = parse_past_performance_xml(xml_file)
        if data is not None:
            # Track statistics
            file_stats[xml_file.name] = {
                'current_races': len(data['current_races']['race_ids']),
                'horses': len(data['past_performances']['horse'])
            }

            # Extend current races data
            for key in current_races:
                current_races[key].extend(data['current_races'][key])

            # Extend past performances data
            for key in past_performances:
                past_performances[key].extend(data['past_performances'][key])

    if not current_races['race_ids']:
        raise ValueError("No current race data was extracted from the XML files")

    # Print summary statistics
    print("\nProcessing Summary:")
    print("=" * 80)
    for filename, stats in file_stats.items():
        print(f"\n{filename}:")
        print(f"  Current races: {stats['current_races']}")
        print(f"  Horses with past performances: {stats['horses']}")

    print("\nOverall Statistics:")
    print(f"Total current races: {len(set(current_races['race_ids']))}")
    print(f"Total horses: {len(set(current_races['horses']))}")
    print("=" * 80)

    # Create xarray dataset for current races
    unique_races = sorted(set(current_races['race_ids']))
    max_starters = max(len([h for h, r in zip(current_races['horses'], current_races['race_ids']) if r == race])
                      for race in unique_races)

    # Create coordinate arrays
    race_coords = np.array(unique_races)
    starter_coords = np.arange(max_starters)

    # Create data arrays for current races
    def create_current_data_array(values, dtype):
        if dtype in ['U35', 'U250', 'U10', 'U5', 'U1']:
            # For string types, ensure we have empty strings instead of None
            values = ['' if x is None else str(x) for x in values]
            arr = np.full((len(unique_races), max_starters), '', dtype=dtype)
        else:
            arr = np.full((len(unique_races), max_starters), np.nan, dtype=dtype)

        for race_idx, race in enumerate(unique_races):
            race_mask = np.array(current_races['race_ids']) == race
            starter_indices = np.arange(len(race_mask))[race_mask]
            race_values = np.array(values)[race_mask]
            arr[race_idx, :len(race_values)] = race_values
        return arr

    # Create data arrays for past performances
    def create_past_data_array(values, dtype):
        # Convert None values to appropriate fill values before creating numpy array
        if dtype == np.int16:
            fill_value = -1
            converted_values = [[fill_value if x is None else x for x in row] for row in values]
        elif dtype == np.float32:
            fill_value = np.nan
            converted_values = [[fill_value if x is None else x for x in row] for row in values]
        elif dtype in ['U35', 'U250', 'U10', 'U5', 'U1']:
            fill_value = ''
            converted_values = [[fill_value if x is None else str(x) if x is not None else fill_value for x in row] for row in values]
        else:
            converted_values = values

        # Create numpy array from converted values
        if dtype in ['U35', 'U250', 'U10', 'U5', 'U1']:
            arr = np.full((len(unique_races), max_starters, 5), '', dtype=dtype)
        else:
            arr = np.full((len(unique_races), max_starters, 5), np.nan, dtype=dtype)

        for race_idx, race in enumerate(unique_races):
            race_mask = np.array(current_races['race_ids']) == race
            starter_indices = np.arange(len(race_mask))[race_mask]
            race_values = np.array(converted_values)[race_mask]
            arr[race_idx, :len(race_values)] = race_values
        return arr

    # Create the dataset
    ds = xr.Dataset(
        {
            # Current race data
            'horse': (['race', 'starter'], create_current_data_array(current_races['horses'], 'U35')),
            'jockey': (['race', 'starter'], create_current_data_array(current_races['jockeys'], 'U250')),
            'trainer': (['race', 'starter'], create_current_data_array(current_races['trainers'], 'U250')),
            'program_number': (['race', 'starter'], create_current_data_array(current_races['program_numbers'], 'U10')),
            'surface': (['race'], np.array([current_races['surfaces'][np.where(np.array(current_races['race_ids']) == race)[0][0]] for race in unique_races], dtype='U1')),
            'distance_f': (['race'], [current_races['distances'][np.where(np.array(current_races['race_ids']) == race)[0][0]] for race in unique_races]),
            'purse': (['race'], [current_races['purses'][np.where(np.array(current_races['race_ids']) == race)[0][0]] for race in unique_races]),

            # Past performance data
            'recent_race_id': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_race_ids'], 'U35')),
            'recent_finish_pos': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_finish_positions'], np.float32)),
            'recent_lengths_back_finish': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_lengths_back_finish'], np.float32)),
            'recent_lengths_back_last_call': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_lengths_back_last_call'], np.float32)),
            'recent_last_call_pos': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_last_call_positions'], np.int16)),
            'recent_surface': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_surfaces'], 'U5')),
            'recent_distance': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_distances'], np.float32)),
            'recent_date': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_dates'], 'U10')),
            'recent_purse': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_purses'], np.float32)),
            'recent_start_pos': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_start_positions'], np.int16)),
            'recent_num_starters': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_numStarters'], np.int16)),
            'recent_jockey': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_jockeys'], 'U250')),
            'recent_trainer': (['race', 'starter', 'past_race'], create_past_data_array(past_performances['recent_trainers'], 'U250'))
        },
        coords={
            'race': race_coords,
            'starter': starter_coords,
            'past_race': np.arange(5)  # 0 = most recent, 4 = 5th most recent
        }
    )

    return ds

def save_dataset(ds, output_dir):
    """Save the xarray dataset to a netCDF file in the specified directory."""
    # Create output directory if it doesn't exist
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Save the dataset
    output_file = output_path / "processed_race_data.nc"
    ds.to_netcdf(output_file)
    print(f"Dataset saved to {output_file}")

if __name__ == "__main__":
    data_dir = "/content/fsan830spring2025/data/rawDataForTraining/pastPerformanceData"

    #data_dir = "data/rawDataForTraining/pastPerformanceData"
    output_dir = "/content/fsan830spring2025/students/neshastehriz_masoud/data/"

    # Create and save the dataset
    ds = create_xarray_dataset(data_dir)
    save_dataset(ds, output_dir)
    print(ds)

import xarray as xr

ds = xr.open_dataset("/content/fsan830spring2025/students/neshastehriz_masoud/data/processed_race_data.nc")
ds

import xarray as xr

# Load the dataset
ds = xr.open_dataset("/content/fsan830spring2025/students/neshastehriz_masoud/data/processed_race_data.nc")

# Print all variables with details
for var_name, var in ds.variables.items():
    print(f"{var_name}: shape={var.shape}, dtype={var.dtype}")

####NEW TRY

import xml.etree.ElementTree as ET
import xarray as xr
import numpy as np
from pathlib import Path
#from parse_race_data import create_xarray_dataset

def parse_race_results_xml(xml_file):
    """Parse a race results XML file and extract race IDs and finish positions."""
    try:
        # Try to parse the XML file with error handling
        try:
            tree = ET.parse(xml_file)
        except ET.ParseError as e:
            print(f"XML parsing error in {xml_file.name}: {str(e)}")
            return None

        root = tree.getroot()

        # Get track and race date information
        track_elem = root.find('.//TRACK')
        track_info = {
            'track_id': track_elem.find('CODE').text if track_elem is not None and track_elem.find('CODE') is not None else 'Unknown',
            'track_name': track_elem.find('NAME').text if track_elem is not None and track_elem.find('NAME') is not None else 'Unknown'
        }

        race_date = root.get('RACE_DATE', 'Unknown')
        # Convert date from YYYY-MM-DD to MM-DD-YY format
        if race_date != 'Unknown':
            year, month, day = race_date.split('-')
            short_year = year[-2:]
            race_date = f"{month}-{day}-{short_year}"

        # Initialize data structures
        race_data = {
            'race_ids': [],
            'horses': [],
            'finish_positions': []
        }

        # Process each race
        for race in root.findall('.//RACE'):
            try:
                race_number = int(race.get('NUMBER', 0))

                # Process each entry in the race
                for entry in race.findall('ENTRY'):
                    try:
                        # Create race ID in the format CD-05-02-23-R01
                        race_id = f"{track_info['track_id']}-{race_date}-R{race_number:02d}"

                        # Get horse information
                        horse_name = entry.find('NAME').text if entry.find('NAME') is not None else ''
                        finish_position = int(entry.find('OFFICIAL_FIN').text) if entry.find('OFFICIAL_FIN') is not None else np.nan

                        # Store the data
                        race_data['race_ids'].append(race_id)
                        race_data['horses'].append(horse_name)
                        race_data['finish_positions'].append(finish_position)

                    except Exception as e:
                        print(f"Error processing entry in race {race_number}: {str(e)}")
                        continue

            except Exception as e:
                print(f"Error processing race {race_number}: {str(e)}")
                continue

        return race_data

    except Exception as e:
        print(f"Error processing {xml_file.name}: {str(e)}")
        return None

def create_race_results_dataset(data_dir):
    """Create an xarray dataset from all race results XML files in the directory."""
    # Initialize data structures
    race_data = {
        'race_ids': [],
        'horses': [],
        'finish_positions': []
    }

    # Process all XML files in the directory
    data_dir_path = Path(data_dir)
    if not data_dir_path.exists():
        raise FileNotFoundError(f"Data directory not found: {data_dir}")

    xml_files = list(data_dir_path.glob('*.xml'))
    if not xml_files:
        raise FileNotFoundError(f"No XML files found in directory: {data_dir}")

    print(f"Found {len(xml_files)} XML files to process")

    # Track statistics for each file
    file_stats = {}

    for xml_file in xml_files:
        print(f"\nProcessing {xml_file.name}...")
        data = parse_race_results_xml(xml_file)
        if data is not None:
            # Track statistics
            file_stats[xml_file.name] = {
                'races': len(set(data['race_ids'])),
                'entries': len(data['race_ids'])
            }

            # Extend data arrays
            for key in race_data:
                race_data[key].extend(data[key])

    if not race_data['race_ids']:
        raise ValueError("No race data was extracted from the XML files")

    # Print summary statistics
    print("\nProcessing Summary:")
    print("=" * 80)
    for filename, stats in file_stats.items():
        print(f"\n{filename}:")
        print(f"  Races: {stats['races']}")
        print(f"  Entries: {stats['entries']}")

    print("\nOverall Statistics:")
    print(f"Total races: {len(set(race_data['race_ids']))}")
    print(f"Total entries: {len(race_data['race_ids'])}")
    print("=" * 80)

    # Create xarray dataset
    unique_races = sorted(set(race_data['race_ids']))
    max_entries = max(len([h for h, r in zip(race_data['horses'], race_data['race_ids']) if r == race])
                     for race in unique_races)

    # Create coordinate arrays
    race_coords = np.array(unique_races)
    entry_coords = np.arange(max_entries)

    # Create data arrays
    def create_data_array(values, dtype):
        if dtype in ['U35', 'U250', 'U10', 'U5', 'U1']:
            # For string types, ensure we have empty strings instead of None
            values = ['' if x is None else str(x) for x in values]
            arr = np.full((len(unique_races), max_entries), '', dtype=dtype)
        else:
            arr = np.full((len(unique_races), max_entries), np.nan, dtype=dtype)

        for race_idx, race in enumerate(unique_races):
            race_mask = np.array(race_data['race_ids']) == race
            entry_indices = np.arange(len(race_mask))[race_mask]
            race_values = np.array(values)[race_mask]
            arr[race_idx, :len(race_values)] = race_values
        return arr

    # Create the dataset
    ds = xr.Dataset(
        {
            'horse': (['race', 'entry'], create_data_array(race_data['horses'], 'U35')),
            'finish_position': (['race', 'entry'], create_data_array(race_data['finish_positions'], np.int16))
        },
        coords={
            'race': race_coords,
            'entry': entry_coords
        }
    )

    return ds

def standardize_race_id(race_id):
    """
    Convert race ID to a standard format: CD-YYYY-MM-DD-RXX
    """
    try:
        # Convert numpy string to Python string
        race_id = str(race_id)

        # Handle format like CD-05-02-23-R01
        if len(race_id.split('-')) == 4:
            track, month, day, rest = race_id.split('-')
            year = '20' + rest[:2]  # Assuming 20xx for year
            race_num = rest[3:]     # Extract R01 from 23-R01
            # Pad month and day with leading zeros if needed
            month = month.zfill(2)
            day = day.zfill(2)
            return f"{track}-{year}-{month}-{day}-{race_num}"

        # Handle format like CD-2023-05-02-R01
        parts = race_id.split('-')
        if len(parts) == 5:
            track, year, month, day, race_num = parts
            # Pad month and day with leading zeros if needed
            month = month.zfill(2)
            day = day.zfill(2)
            return f"{track}-{year}-{month}-{day}-{race_num}"

        return race_id
    except Exception as e:
        print(f"Error standardizing race ID {race_id}: {str(e)}")
        return race_id

def normalize_horse_name(name):
    """Normalize horse name by removing 'dh-' prefix and standardizing case."""
    if not name:
        return name
    name = str(name).strip()
    if name.lower().startswith('dh-'):
        name = name[3:]
    return name

def is_dead_heat(name):
    """Check if a horse name has the dead heat prefix."""
    if not name:
        return False
    return str(name).lower().startswith('dh-')

def merge_race_data_with_results(race_ds, results_ds):
    """
    Merge the race data dataset with the race results dataset.

    Parameters:
    -----------
    race_ds : xarray.Dataset
        The dataset from parse_race_data.py containing race and past performance information
    results_ds : xarray.Dataset
        The dataset from create_race_results_dataset containing race results

    Returns:
    --------
    xarray.Dataset
        The merged dataset with finish positions added to the race data
    """
    print("\nMerging datasets...")
    print(f"Race dataset has {len(race_ds.race)} races")
    print(f"Results dataset has {len(results_ds.race)} races")

    # Standardize race IDs in both datasets
    race_ids_standardized = [standardize_race_id(race_id) for race_id in race_ds.race.values]
    results_race_ids_standardized = [standardize_race_id(race_id) for race_id in results_ds.race.values]

    # Create a mapping of standardized race IDs to their indices in the results dataset
    results_race_map = {race_id: idx for idx, race_id in enumerate(results_race_ids_standardized)}

    # Initialize arrays
    finish_positions = np.full_like(race_ds.horse, np.nan, dtype=np.int16)
    scratched = np.zeros_like(race_ds.horse, dtype=bool)  # True if horse was scratched

    # Track statistics
    matched_races = 0
    matched_horses = 0
    total_horses = 0
    scratched_horses = 0
    dead_heat_count = 0
    unmatched_horses = []

    # For each race in the race dataset
    for race_idx, race_id in enumerate(race_ids_standardized):
        if race_id in results_race_map:
            matched_races += 1
            # Get the corresponding race from results
            results_race_idx = results_race_map[race_id]

            # Get the horses and finish positions for this race
            results_horses = results_ds.horse[results_race_idx].values
            results_finish_positions = results_ds.finish_position[results_race_idx].values

            # Create a mapping of normalized horse names to their finish positions
            horse_finish_map = {}
            dead_heat_groups = {}  # Track groups of horses in dead heats

            # First pass: identify dead heat groups
            for horse, pos in zip(results_horses, results_finish_positions):
                if horse != '' and not np.isnan(pos):
                    normalized_horse = normalize_horse_name(horse)
                    if is_dead_heat(horse):
                        if pos not in dead_heat_groups:
                            dead_heat_groups[pos] = []
                        dead_heat_groups[pos].append(normalized_horse)
                        dead_heat_count += 1
                    horse_finish_map[normalized_horse] = pos

            # Second pass: ensure all horses in dead heat groups have the same position
            for pos, horses in dead_heat_groups.items():
                for horse in horses:
                    horse_finish_map[horse] = pos

            # For each horse in the race dataset
            for entry_idx, horse in enumerate(race_ds.horse[race_idx].values):
                # Only count non-empty entries
                if horse != '':
                    total_horses += 1
                    normalized_horse = normalize_horse_name(horse)
                    if normalized_horse in horse_finish_map:
                        matched_horses += 1
                        finish_positions[race_idx, entry_idx] = horse_finish_map[normalized_horse]
                    else:
                        # If horse is in race dataset but not in results, it was scratched
                        scratched[race_idx, entry_idx] = True
                        scratched_horses += 1
                        unmatched_horses.append((race_id, horse))
                        # Print detailed comparison for the first few scratched horses
                        if len(unmatched_horses) <= 3:
                            print(f"\nDetailed comparison for race {race_id}:")
                            print("Race dataset horses:")
                            for h in race_ds.horse[race_idx].values:
                                if h != '':
                                    print(f"  - {h}")
                            print("\nResults dataset horses:")
                            for h in results_horses:
                                if h != '':
                                    print(f"  - {h}")

    # Print matching statistics
    print("\nMatching Statistics:")
    print(f"Matched races: {matched_races} out of {len(race_ds.race)}")
    print(f"Total actual entries: {total_horses}")  # Changed from total_horses to total actual entries
    print(f"Matched horses: {matched_horses} out of {total_horses}")
    print(f"Scratched horses: {scratched_horses}")
    print(f"Dead heats found: {dead_heat_count}")

    # Print sample of scratched horses for debugging
    if unmatched_horses:
        print("\nSample of scratched horses:")
        for race_id, horse in unmatched_horses[:10]:
            print(f"Race {race_id}: {horse}")

    # Add finish positions and scratched flag to the race dataset
    race_ds['finish_position'] = (['race', 'starter'], finish_positions)
    race_ds['scratched'] = (['race', 'starter'], scratched)

    return race_ds

def save_dataset(ds, output_dir):
    """Save the xarray dataset to a netCDF file in the specified directory."""
    # Create output directory if it doesn't exist
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Save the dataset
    output_file = output_path / "processed_race_data_with_results.nc"
    ds.to_netcdf(output_file)
    print(f"Dataset saved to {output_file}")

import xarray as xr

race_ds = xr.open_dataset("/content/fsan830spring2025/students/neshastehriz_masoud/data/processed_race_data.nc")

results_ds = create_race_results_dataset("/content/fsan830spring2025/data/rawDataForTraining/resultsData")

merged_ds = merge_race_data_with_results(race_ds, results_ds)

merged_ds.to_netcdf("/content/fsan830spring2025/students/neshastehriz_masoud/data/processed_race_data_with_results.nc")
print("✅ Merged dataset saved!")

import xarray as xr

# Load the merged dataset
ds = xr.open_dataset("/content/fsan830spring2025/students/neshastehriz_masoud/data/processed_race_data_with_results.nc")

# Show a summary of the dataset
print(ds)

# List all variables (features)
print("\nVariables:")
print(list(ds.variables))

import xarray as xr
import pandas as pd

# Load the merged dataset
ds = xr.open_dataset("/content/fsan830spring2025/students/neshastehriz_masoud/data/processed_race_data_with_results.nc")

# Initialize a list to store all rows
data_rows = []

# Loop over each race and starter
for race_idx in range(ds.sizes['race']):
    for starter_idx in range(ds.sizes['starter']):
        horse_name = ds['horse'][race_idx, starter_idx].values
        if horse_name == '':  # Skip empty entries
            continue

        finish_position = ds['finish_position'][race_idx, starter_idx].values
        is_winner = 1 if finish_position == 1 else 0

        row = {
            "race_id": ds['race'].values[race_idx],
            "starter_index": starter_idx,
            "horse": horse_name,
            "jockey": ds['jockey'][race_idx, starter_idx].values,
            "is_winner": is_winner
        }

        # Extract features for past races
        for i in range(5):
            row[f"finish_pos_{i+1}"] = ds['recent_finish_pos'][race_idx, starter_idx, i].values.item()
            row[f"lengths_back_{i+1}"] = ds['recent_lengths_back_finish'][race_idx, starter_idx, i].values.item()
            row[f"distance_{i+1}"] = ds['recent_distance'][race_idx, starter_idx, i].values.item()
            row[f"surface_{i+1}"] = ds['recent_surface'][race_idx, starter_idx, i].values.item()

        data_rows.append(row)

# Create DataFrame
df = pd.DataFrame(data_rows)

# Show the first few rows
print(df.head())

import xarray as xr
import pandas as pd
import numpy as np

# Load the merged dataset
ds = xr.open_dataset("/content/fsan830spring2025/students/neshastehriz_masoud/data/processed_race_data_with_results.nc")

# Initialize a list to store all rows
data_rows = []

# Loop over each race and starter
for race_idx in range(ds.sizes['race']):
    for starter_idx in range(ds.sizes['starter']):
        horse_name = ds['horse'][race_idx, starter_idx].values
        if horse_name == '':
            continue

        finish_position = ds['finish_position'][race_idx, starter_idx].values
        is_winner = 1 if finish_position == 1 else 0

        # Get feature arrays (convert to numpy to handle NaNs)
        finish_pos = ds['recent_finish_pos'][race_idx, starter_idx, :].values.astype(float)
        lengths_back = ds['recent_lengths_back_finish'][race_idx, starter_idx, :].values.astype(float)
        distances = ds['recent_distance'][race_idx, starter_idx, :].values.astype(float)

        # For surface (categorical), we will take the most frequent one
        surfaces = ds['recent_surface'][race_idx, starter_idx, :].values
        surface_mode = pd.Series(surfaces[surfaces != '']).mode()
        surface_agg = surface_mode.iloc[0] if not surface_mode.empty else ''

        row = {
            "race_id": ds['race'].values[race_idx],
            "starter_index": starter_idx,
            "horse": horse_name,
            "jockey": ds['jockey'][race_idx, starter_idx].values,
            "is_winner": is_winner,
            "avg_finish_pos": np.nanmean(finish_pos),
            "avg_lengths_back": np.nanmean(lengths_back),
            "avg_distance": np.nanmean(distances),
            "most_common_surface": surface_agg
        }

        data_rows.append(row)

# Create DataFrame
df_avg = pd.DataFrame(data_rows)

# Show first few rows
print(df_avg.head())

import xarray as xr
import pandas as pd
import numpy as np

# Load the dataset
ds = xr.open_dataset("/content/fsan830spring2025/students/neshastehriz_masoud/data/processed_race_data_with_results.nc")

rows = []
for race_idx in range(ds.sizes['race']):
    for starter_idx in range(ds.sizes['starter']):
        horse = ds['horse'][race_idx, starter_idx].values
        if isinstance(horse, (bytes, np.bytes_)):
            horse = horse.decode("utf-8")
        if horse == "":
            continue

        finish_position = ds['finish_position'][race_idx, starter_idx].values
        if np.isnan(finish_position):  # skip if finish position is missing
            continue
        is_winner = 1 if finish_position == 1 else 0

        # Averages of past features
        finish_pos = ds['recent_finish_pos'][race_idx, starter_idx, :].values
        lengths_back = ds['recent_lengths_back_finish'][race_idx, starter_idx, :].values
        distance = ds['recent_distance'][race_idx, starter_idx, :].values

        # Surface (most common)
        surfaces = ds['recent_surface'][race_idx, starter_idx, :].values
        surfaces = [s.decode() if isinstance(s, bytes) else s for s in surfaces]
        surfaces = [s for s in surfaces if s]  # remove empty strings
        surface = pd.Series(surfaces).mode()[0] if surfaces else "Unknown"

        # Jockey (decode if needed)
        jockey = ds['jockey'][race_idx, starter_idx].values
        if isinstance(jockey, (np.ndarray, bytes)):
            jockey = jockey.decode("utf-8") if isinstance(jockey, bytes) else str(jockey)

        rows.append({
            "race_id": str(ds['race'].values[race_idx]),
            "starter_index": starter_idx,
            "horse": horse,
            "jockey": jockey,
            "avg_finish_pos": np.nanmean(finish_pos),
            "avg_lengths_back": np.nanmean(lengths_back),
            "avg_distance": np.nanmean(distance),
            "most_common_surface": surface,
            "is_winner": is_winner
        })

# Build DataFrame
df = pd.DataFrame(rows)

# One-hot encode and convert to 0/1 integers
df_encoded = pd.get_dummies(df, columns=["most_common_surface", "jockey"], drop_first=True)
df_encoded = df_encoded.astype(int, errors='ignore')  # Convert bools to int

# Display result
print(df_encoded.head())

import pandas as pd
from pathlib import Path

# File paths
data_dir = Path("/content/fsan830spring2025/students/neshastehriz_masoud/data/")
test_path = data_dir / "CDX0515.csv"
mapping_path = data_dir / "column_mapping.csv"
output_path = data_dir / "CDX0515_with_header.csv"

# Load data
test_df = pd.read_csv(test_path, header=None)
mapping_df = pd.read_csv(mapping_path)

# Build header labels
column_name_map = mapping_df.set_index("column_number")["header_name"].to_dict()
header_labels = []

for i in range(1, test_df.shape[1] + 1):  # column_number is 1-based
    header = column_name_map.get(i)
    if pd.isna(header) or header.strip() == "":
        header_labels.append(f"column_{i}")
    else:
        header_labels.append(header)

# Insert header as a new first row
header_df = pd.DataFrame([header_labels], columns=test_df.columns)
test_df_with_header = pd.concat([header_df, test_df], ignore_index=True)

# Save updated file
test_df_with_header.to_csv(output_path, index=False, header=False)
print(f"Saved with header row at: {output_path}")

import pandas as pd

# Load your test file
test_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/CDX0515_with_header.csv")

# Step 1: Calculate average finish position
finish_pos_cols = [col for col in test_df.columns if "finish" in col.lower() and "position" in col.lower()]
test_df[finish_pos_cols] = test_df[finish_pos_cols].apply(pd.to_numeric, errors='coerce')
test_df["avg_finish_pos"] = test_df[finish_pos_cols].mean(axis=1)

# Step 2: Calculate average lengths back at finish
lengths_back_cols = [col for col in test_df.columns if "length" in col.lower()]
test_df[lengths_back_cols] = test_df[lengths_back_cols].apply(pd.to_numeric, errors='coerce')
test_df["avg_lengths_back"] = test_df[lengths_back_cols].mean(axis=1)

# Step 3: Calculate average race distance
distance_cols = [col for col in test_df.columns if "distance" in col.lower()]
test_df[distance_cols] = test_df[distance_cols].apply(pd.to_numeric, errors='coerce')
test_df["avg_distance"] = test_df[distance_cols].mean(axis=1)

# Step 4: Surface
surface_cols = [col for col in test_df.columns if "surface" in col.lower()]
# Pick first non-null surface
test_df["most_common_surface"] = test_df[surface_cols].bfill(axis=1).iloc[:, 0]

# Step 5: Jockey
jockey_cols = [col for col in test_df.columns if "jockey" in col.lower()]
test_df["jockey"] = test_df[jockey_cols].bfill(axis=1).iloc[:, 0]

# Step 6: Keep only final features
test_features = test_df[["avg_finish_pos", "avg_lengths_back", "avg_distance", "most_common_surface", "jockey"]]

# Step 7: One-hot encode surface and jockey
test_features_encoded = pd.get_dummies(test_features, columns=["most_common_surface", "jockey"], drop_first=True)

# DONE! ✅
print(test_features_encoded.head())

# Assuming df_encoded is still in your notebook memory
df_encoded.to_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/train_encoded.csv", index=False)

print("Training encoded data saved successfully.")

import pandas as pd

# Load your saved df_encoded (training data)
train_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/train_encoded.csv")

# List of features you want to pivot for each starter
features_to_pivot = ["avg_finish_pos", "avg_lengths_back", "avg_distance", "is_winner"]

# Pivot the data
pivoted = train_df.pivot(index="race_id", columns="starter_index", values=features_to_pivot)

# Flatten MultiIndex columns
pivoted.columns = [f"st{starter_idx+1}_{feature}" for feature, starter_idx in pivoted.columns]

# Reset index to bring race_id back as a column
pivoted = pivoted.reset_index()

print("Shape of pivoted training data:", pivoted.shape)
print(pivoted.head())

# Save for using in your BART code
pivoted.to_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/training_features.csv", index=False)

import pandas as pd
import numpy as np

# Load your test set
test_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/CDX0515_with_header.csv")

# TEMPORARY: Create fake 'race_id' column (since only 1 race)
test_df["race_id"] = 1

# TEMPORARY: Create 'starter_index' (just 0, 1, 2, ..., n)
test_df["starter_index"] = np.arange(len(test_df))

# Create fake "past" data columns manually, or extract from what you have.
# Here, you should try to find columns like 'finish1', 'finish2', ..., or similar.

# Example: Select columns related to past finish positions
past_finish_cols = [col for col in test_df.columns if "finish" in col.lower()]

# Calculate average past finish position
test_df["avg_finish_pos"] = test_df[past_finish_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)

# Similarly for lengths back (if you have these columns)
past_lengths_back_cols = [col for col in test_df.columns if "lengths" in col.lower()]

test_df["avg_lengths_back"] = test_df[past_lengths_back_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)

# Similarly for distance (if available)
past_distance_cols = [col for col in test_df.columns if "distance" in col.lower()]

test_df["avg_distance"] = test_df[past_distance_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)

# Assume "surface" column exists or fake it if not
if "surface" in test_df.columns:
    test_df["most_common_surface"] = test_df["surface"]
else:
    test_df["most_common_surface"] = "Unknown"  # default if surface not available

# Now select the correct columns
features_to_keep = ["race_id", "starter_index", "avg_finish_pos", "avg_lengths_back", "avg_distance", "most_common_surface"]

# Build clean test set
test_clean = test_df[features_to_keep]

# One-hot encode surface (match train)
test_clean = pd.get_dummies(test_clean, columns=["most_common_surface"], drop_first=True)

# Pivot the test data to starter columns
pivoted_test = test_clean.pivot(index="race_id", columns="starter_index")

# Flatten the columns
pivoted_test.columns = [f"st{starter_idx+1}_{feature}" for feature, starter_idx in pivoted_test.columns]

# Reset index
pivoted_test = pivoted_test.reset_index()

print("Pivoted Test Data:")
print(pivoted_test.head())

# Save it
pivoted_test.to_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/prediction_features.csv", index=False)

!pip install pymc-bart

import pandas as pd
import numpy as np

# Load your existing training data
train_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/train_encoded.csv")

# Number of starters per race
n_starters = 14

# Initialize new target columns
for i in range(1, n_starters + 1):
    train_df[f"st{i}_pts"] = 0

# Now set the corresponding starter's column to 1 if is_winner == 1
for idx, row in train_df.iterrows():
    starter_idx = int(row['starter_index']) + 1  # Assuming starter_index starts from 0
    if row['is_winner'] == 1:
        train_df.at[idx, f'st{starter_idx}_pts'] = 1

# Done! Now train_df has 14 "st*_pts" columns for targets

# Save it if you want
train_df.to_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/train_with_targets.csv", index=False)

print("✅ Target columns added successfully!")

import pandas as pd
import numpy as np

# Load train and test datasets
train_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/train_encoded.csv")
test_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/CDX0515_with_header.csv")

# ====================================
# 1. Feature Engineering on Test Set
# ====================================

# Step 1: Calculate avg_finish_pos
finish_cols = [col for col in test_df.columns if 'finish' in col.lower() and 'position' in col.lower()]
test_df['avg_finish_pos'] = test_df[finish_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)

# Step 2: Calculate avg_lengths_back
lengths_cols = [col for col in test_df.columns if 'length' in col.lower() and 'finish' in col.lower()]
test_df['avg_lengths_back'] = test_df[lengths_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)

# Step 3: Calculate avg_distance
distance_cols = [col for col in test_df.columns if 'distance' in col.lower()]
test_df['avg_distance'] = test_df[distance_cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)

# Step 4: Extract most_common_surface
surface_cols = [col for col in test_df.columns if 'surface' in col.lower()]
def most_common_surface(row):
    surfaces = row[surface_cols].dropna().astype(str)
    if len(surfaces) == 0:
        return "Unknown"
    return surfaces.mode()[0]

test_df['most_common_surface'] = test_df.apply(most_common_surface, axis=1)

# Step 5: Extract jockey
# Find column containing jockey info
jockey_cols = [col for col in test_df.columns if 'jockey' in col.lower()]
if jockey_cols:
    test_df['jockey'] = test_df[jockey_cols[0]].astype(str)
else:
    test_df['jockey'] = "Unknown"

# ====================================
# 2. Select Only Final Test Features
# ====================================

# Keep same columns as in training
final_test_df = test_df[['avg_finish_pos', 'avg_lengths_back', 'avg_distance', 'most_common_surface', 'jockey']]

# ====================================
# 3. One-hot Encode Categorical Features
# ====================================

# Perform one-hot encoding
test_encoded = pd.get_dummies(final_test_df, columns=['most_common_surface', 'jockey'], drop_first=True)

# ====================================
# 4. Align Test Columns with Train Columns
# ====================================

# Load training columns (excluding target)
train_features = train_df.drop(columns=['race_id', 'starter_index', 'horse', 'is_winner']).columns

# Add missing columns in test set
for col in train_features:
    if col not in test_encoded.columns:
        test_encoded[col] = 0  # Add missing columns with zeros

# Keep only training features and order them
test_encoded = test_encoded[train_features]

# ====================================
# 5. Save Cleaned Test Data
# ====================================

# Save it if you want
test_encoded.to_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/test_encoded.csv", index=False)

print("✅ Test data processed and saved successfully!")
print(test_encoded.head())

# Fill NaN values with 0 (important for PyMC)
X_train = np.nan_to_num(X_train, nan=0.0)
X_pred = np.nan_to_num(X_pred, nan=0.0)

# Now build the model
with pm.Model() as model:
    X = pm.Data("X", X_train)
    μ = pmb.BART("μ", X, y_train, m=50)
    σ = pm.HalfNormal("σ", 1)

    y_obs = pm.Normal("y_obs", mu=μ, sigma=σ, observed=y_train)

import pandas as pd

# Load your test_encoded.csv
test_encoded = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/test_encoded.csv")

# Create dummy race_id and starter_index
n_rows = len(test_encoded)
n_starters_per_race = 14  # adjust if you know the real number!

race_ids = []
starter_indices = []

for i in range(n_rows):
    race_id = i // n_starters_per_race  # simple numbering
    starter_idx = i % n_starters_per_race
    race_ids.append(f"race_{race_id}")
    starter_indices.append(starter_idx)

# Add them
test_encoded.insert(0, "race_id", race_ids)
test_encoded.insert(1, "starter_index", starter_indices)

# Create a fake horse name too
test_encoded.insert(2, "horse", [f"horse_{i}" for i in range(n_rows)])

# Save new version
test_encoded.to_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/test_encoded_full.csv", index=False)

print("✅ Created fake IDs and saved test_encoded_full.csv!")

import pandas as pd
import numpy as np
import pymc as pm
import pymc_bart as pmb
import arviz as az
import os

# 1. Load the training and test sets
train_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/train_encoded.csv")
test_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/test_encoded_full.csv")

# 2. Feature columns
feature_cols = [col for col in train_df.columns if col not in ['race_id', 'starter_index', 'horse', 'is_winner']]

# 3. Handle missing data
# Drop rows where target is missing
train_df = train_df.dropna(subset=['is_winner'])
# Fill missing feature values with 0
train_df[feature_cols] = train_df[feature_cols].fillna(0)

# 4. Prepare X and y
X_train = train_df[feature_cols].values
y_train = train_df['is_winner'].values

X_test = test_df[feature_cols].fillna(0).values
test_race_ids = test_df['race_id'].values
test_starter_indices = test_df['starter_index'].values
test_horses = test_df['horse'].values

# 5. Train the BART model
with pm.Model() as model:
    X_data = pm.Data("X_data", X_train)
    y_data = pm.Data("y_data", y_train)

    μ = pmb.BART("μ", X_data, y_data, m=50)
    σ = pm.HalfNormal("σ", 1.0)

    y_obs = pm.Normal("y_obs", mu=μ, sigma=σ, observed=y_data)

    idata = pm.sample(1000, tune=1000, chains=4, cores=4, random_seed=42)

# 6. Predict on the test set
with model:
    pm.set_data({"X_data": X_test})
    pred_idata = pm.sample_posterior_predictive(idata, var_names=["μ"])

# 7. Extract mean predictions
mu_preds = pred_idata.posterior_predictive["μ"].mean(dim=["chain", "draw"]).values

# 8. Build prediction DataFrame
pred_df = pd.DataFrame({
    "race_id": test_race_ids,
    "starter_index": test_starter_indices,
    "horse": test_horses,
    "predicted_win_probability": mu_preds
})

# 9. Save the prediction results
output_path = "/content/fsan830spring2025/students/neshastehriz_masoud/data/test_predictions.csv"
pred_df.to_csv(output_path, index=False)

print(f"✅ Saved predictions to {output_path}")
print(pred_df.head())

import pandas as pd
import numpy as np
import pymc as pm
import pymc_bart as pmb
import arviz as az
import os

# 1. Load the training and test sets
train_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/train_encoded.csv")
test_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/test_encoded_full.csv")

# 2. Define feature columns
feature_cols = [col for col in train_df.columns if col not in ['race_id', 'starter_index', 'horse', 'is_winner']]

# 3. Handle missing data
train_df = train_df.dropna(subset=['is_winner'])
train_df[feature_cols] = train_df[feature_cols].fillna(0)
train_df[feature_cols] = train_df[feature_cols].astype(np.float32)

# Test set cleaning
test_df[feature_cols] = test_df[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(0)
test_df[feature_cols] = test_df[feature_cols].astype(np.float32)

# 4. Prepare data
X_train = train_df[feature_cols].values
y_train = train_df['is_winner'].values

X_test = test_df[feature_cols].values
test_race_ids = test_df['race_id'].astype(str).values  # ensure race_id is string
test_starter_indices = test_df['starter_index'].values
test_horses = test_df['horse'].values

# 5. Build and train BART model
with pm.Model() as model:
    X_data = pm.Data("X_data", X_train)
    y_data = pm.Data("y_data", y_train)

    μ = pmb.BART("μ", X_data, y_data, m=50)
    σ = pm.HalfNormal("σ", 1.0)

    y_obs = pm.Normal("y_obs", mu=μ, sigma=σ, observed=y_data)

    idata = pm.sample(1000, tune=1000, chains=4, cores=4, random_seed=42)

# 6. Predict on test set
with model:
    pm.set_data({"X_data": X_test})
    pred_idata = pm.sample_posterior_predictive(idata, var_names=["μ"])

# 7. Extract predictions
mu_preds = pred_idata.posterior_predictive["μ"].mean(dim=["chain", "draw"]).values

# 8. Build predictions DataFrame
pred_df = pd.DataFrame({
    "race_id": test_race_ids,
    "starter_index": test_starter_indices,
    "horse": test_horses,
    "predicted_win_probability": mu_preds
})

# 9. Save predictions
output_path = "/content/fsan830spring2025/students/neshastehriz_masoud/data/test_predictions.csv"
pred_df.to_csv(output_path, index=False)

print(f"✅ Saved predictions to {output_path}")
print(pred_df.head())

# Find the winner per race
winners = pred_df.loc[pred_df.groupby("race_id")["predicted_win_probability"].idxmax()]

# Save winners
winners.to_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/final_race_winners.csv", index=False)

print(winners)

import pandas as pd

# Load your predictions
pred_df = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/test_predictions.csv")

# Find the winner per race
winners = pred_df.loc[pred_df.groupby("race_id")["predicted_win_probability"].idxmax()]

# Save to CSV
winners_path = "/content/fsan830spring2025/students/neshastehriz_masoud/data/final_race_winners.csv"
winners.to_csv(winners_path, index=False)

print(f"✅ Saved final winners to {winners_path}")

# Show winners
print(winners[["race_id", "starter_index", "horse", "predicted_win_probability"]])

import pandas as pd

# Load your prediction winners
winners = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/final_race_winners.csv")

# Load the original test file (with real horse names)
original_test = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/CDX0515_with_header.csv")

# Ensure 'race_id' is string type
winners["race_id"] = winners["race_id"].astype(str)

# Create matching race_id and starter_index in the original file
original_test["starter_index"] = original_test.groupby(["race_number"]).cumcount()
original_test["race_id"] = "race_" + original_test.groupby(["race_number"]).ngroup().astype(str)

# Extract the horse name (column 44 -> index 43)
horse_names = original_test[["race_id", "starter_index", original_test.columns[44]]]
horse_names.columns = ["race_id", "starter_index", "real_horse_name"]

# Merge winners with horse names
final_winners = winners.merge(horse_names, on=["race_id", "starter_index"], how="left")

# Save final file
final_path = "/content/fsan830spring2025/students/neshastehriz_masoud/data/final_race_winners_with_names.csv"
final_winners.to_csv(final_path, index=False)

print(f"✅ Saved final winners WITH horse names to {final_path}")

# Show nicely
print(final_winners[["race_id", "starter_index", "real_horse_name", "predicted_win_probability"]])

import pandas as pd

# Load winners
winners = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/final_race_winners.csv")

# Load the original test set (with real horse names, assuming here)
raw_test = pd.read_csv("/content/fsan830spring2025/students/neshastehriz_masoud/data/CDX0515_with_header.csv", header=0)

# In your CDX0426_with_header.csv, real horse name is in column 44
# Let's check
horse_name_col_index = 44  # Python indexing is zero-based

# Build real horse names DataFrame
real_horse_names = pd.DataFrame({
    "race_id": [f"race_{i//14}" for i in range(len(raw_test))],  # If 14 starters per race
    "starter_index": [i % 14 for i in range(len(raw_test))],
    "real_horse_name": raw_test.iloc[:, horse_name_col_index].values
})

# Merge real horse names into winners
final_winners = winners.merge(real_horse_names, on=["race_id", "starter_index"], how="left")

# Save
final_output_path = "/content/fsan830spring2025/students/neshastehriz_masoud/data/final_winners_with_real_names.csv"
final_winners.to_csv(final_output_path, index=False)

print(f"✅ Saved updated winners with real horse names to {final_output_path}")
print(final_winners.head())#done!